# HAZEL â€” A Context-Aware Mixed Reality Voice Assistant

## MADE FOR MIXED REALITY WEARABLES LIKE META RAY BAN GLASSES AND META OCULUS

HAZEL is an intelligent voice-driven mixed reality assistant capable of understanding your environment, responding conversationally, and generating realistic speech in real time. Think JARVIS inside MR â€” aware of surroundings, answering questions, narrating insights, guiding actions, and interacting naturally.

---

## âœ¨ Core Features

### ğŸ”¹ Spatial Understanding
- Continuously scans and interprets the user's environment
- Recognizes surfaces, objects, markers, or scene elements
- Users can ask questions about visible surroundings
- Extensible into overlay guidance and contextual MR workflows

### ğŸ”¹ Push-to-Talk Conversational Interaction
- A simple button press activates voice listening
- Speak naturally after activation
- Responds intelligently using scene awareness + general reasoning
- Maintains dialogue flow across follow-up questions

### ğŸ”¹ Real-Time Voice Output with Murf Falcon TTS
- Produces natural-sounding responses immediately using **Murf Falcon TTS**
- High-quality, human-like voice synthesis
- Enables smooth, immersive, human-like communication
- Customizable voice parameters (pitch, speed, speaker selection)

### ğŸ”¹ Hands-Free Playback
- Once activated via button, interaction is spoken end-to-end
- No typing or UI navigation required
- Ideal for accessibility, spatial guidance, and task automation

### ğŸ”¹ Secure API Usage
- API keys handled through environment variables
- Clean separation between codebase and credentials
- No hardcoded secrets in repository

---

## ğŸ§  Architecture Overview

```
Environment Scanner  â†’  Scene Context Engine  
        â†“                          â†“
Push-to-Talk Input â†’ Speech Recognition â†’ AI Reasoning â†’ Voice Synthesis (Murf Falcon TTS) â†’ Audio Output
```

**Scene perception + spoken query = grounded, contextual intelligent answers.**

---

## ğŸ” Vision + Voice Reasoning Pipeline

### ğŸ§  OpenAI Vision Model Integration

**Purpose:** Query a multi-modal LLM (Vision + Language) for contextual reasoning of the current scene.

#### How It Works:

**1. Speech Input â†’ Command String**
- User triggers input via push-to-talk
- Microphone audio is captured and sent to OpenAI Speech-to-Text (ASR)
- The resulting text becomes the query instruction for the vision model

**2. Scene Capture â†’ Screenshot Feed**
- A camera feed (Unity render texture / mixed reality viewport capture) is captured
- The frame is serialized and attached as a binary payload to the vision request

**3. Multi-Modal LLM Request**
- The system sends a combined payload:
  - Command text (intent/question)
  - Scene image (snapshot)
  - Optional additional prompting context
- Supported request modes:
  - Text-only
  - Image-only
  - Image + Command Text hybrid reasoning

**4. Inference (Vision Model Reasoning)**
- The model returns structured natural-language output containing:
  - Object recognition
  - Affordance suggestions
  - Spatial descriptions
  - Task answers
  - Conversational reasoning
- End-to-end round-trip typically runs 2â€“6 seconds, depending on network conditions

**5. Speech Output (Murf Falcon TTS Synthesis)**
- The returned text is routed to **Murf Falcon TTS** for voice generation
- High-quality, natural-sounding speech synthesis
- The result is streamed into Unity's audio playback system
- The user hears the answer spoken aloud

---

## âš™ï¸ Configurable Parameters

The system supports runtime adjustments for:

- **Speaker voice selection** (via Murf Falcon TTS)
- **Speech pitch and playback speed**
- **Model selection** (different OpenAI vision/language model variants)
- **Prompt modifiers:**
  - Task framing
  - System instructions
  - Domain-specific guidance

Developers can dynamically modify the model's behavior by changing prompt templates and image pairing strategies.

---

## ğŸ“Œ Technology Stack

- âœ” **Unity** â€” Mixed reality application platform
- âœ” **Spatial mapping and perception components**
- âœ” **Push-to-talk speech recognition**
- âœ” **OpenAI Vision API** â€” Multi-modal AI reasoning
- âœ” **OpenAI Speech-to-Text (Whisper)** â€” Voice input processing
- âœ” **Murf Falcon TTS** â€” Natural voice synthesis
- âœ” **Dialogue and perception intelligence system**

---

## ğŸŒ Possible Applications

HAZEL can evolve into:

- âœ” Mixed Reality assistant
- âœ” Guided exploration companion
- âœ” Smart classroom/tutorial agent
- âœ” Interactive environment narrator
- âœ” Voice-based productivity tool
- âœ” Accessibility support and narration aid

---

## ğŸ¯ Why Mixed Reality + Push-to-Talk Voice?

- **Voice enables natural communication**
- **Button triggering ensures intentional interaction**
- **MR context increases relevance of answers**
- **Makes digital assistance physically present in your space**

---

## ğŸ“¢ Inspiration

A step toward **JARVIS-like ambient intelligence** â€” an assistant that listens when called, understands space, and responds meaningfully.

---

## ğŸš€ Getting Started

### Prerequisites
- Unity 2021.3 or later
- OpenAI API key
- Murf Falcon TTS API access
- Mixed Reality headset (Quest, HoloLens, etc.)

### Setup
1. Clone the repository
2. Configure environment variables for API keys:
   ```bash
   OPENAI_API_KEY=your_openai_key
   MURF_API_KEY=your_murf_key
   ```
3. Open project in Unity
4. Deploy to your MR device
5. Press the push-to-talk button and start conversing!
---



---

**Built with â¤ï¸ for the future of spatial computing*
